{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sweet Lift Taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet Lift Taxi company has collected data on taxi orders at airports. Their aim is to predict the amount of taxi orders for the next hour, in order to allocate more drivers for peak hours. We will build a model with an RMSE lower than 48. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --user plotly_express"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import plotly_express as px \n",
    "from matplotlib import pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, VotingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as mse, mean_absolute_error as mae, make_scorer\n",
    "from xgboost import XGBRegressor \n",
    "from lightgbm import LGBMRegressor \n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor, Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataframe\n",
    "df = pd.read_csv('datasets/taxi.csv', parse_dates=['datetime'], index_col=['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting index\n",
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if index is monotonic\n",
    "print(df.index.is_monotonic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info on num orders columns\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming no missing values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample data by the hour\n",
    "df = df.resample('1H').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We loaded the data and then converted the dates into datetime format. We then made the datetime column our index, and sorted the index. We checked to make sure the data was free of missing values. Following that, we resampled the data by the hour. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics on the number of orders\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hourly number of orders\n",
    "fig = px.line(df.num_orders, title='Total Hourly Number of Orders', template='ggplot2', height=600, labels={'value': 'Number of Orders'})\n",
    "fig.update_xaxes(rangeslider_visible=True, \n",
    "    rangeselector=dict(\n",
    "        buttons=list([\n",
    "            dict(count=1, label='1m', step='month', stepmode='backward'), \n",
    "            dict(count=6, label='6m', step='month', stepmode='backward')\n",
    "            ])\n",
    "        )\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a visual of the timeseries data sampled on the hour. The y axis shows the number of taxi orders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of orders\n",
    "px.box(df.num_orders, title='Distribution of Orders', template='ggplot2', labels={'variable': 'Orders', 'value': 'count'}, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we have the distribution of the number of orders. We see some outliers with values above 186. The average is 78 orders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average daily orders\n",
    "numbers = [6, 12, 24, 168, 720] \n",
    "for i in numbers:\n",
    "    px.line(df.num_orders.rolling(i).mean(), title=f'Mean Number of Orders per {i} Hours', template='ggplot2', labels={'value': 'Number of Orders', 'datetime':'Dates'}, height=600).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These visuals show the number of orders resampled for 6 hours, 12 hours, per day, per week, and per month. These visualizations allow us the clearly see the trend in orders increase gradually from April to August. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposed dataset\n",
    "decomposed = seasonal_decompose(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposed trend \n",
    "px.line(decomposed.trend, title='Trend', template='ggplot2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposed seasonality \n",
    "px.line(decomposed.seasonal, title='Seasonality', template='ggplot2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decomposed residual \n",
    "px.line(decomposed.resid, title='Residual', template='ggplot2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These visualizations show us the decomposed trends of the data. The trend is the same chart as the resampled daily chart. We see the seasonality chart is tight, which may be due to the short window of time given by the data. The residuals generally fluctuate around 0, but starts to show outliers in August. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# difference in number of orders\n",
    "fig = px.line(df.num_orders-df.num_orders.shift(), title='Difference in Number of Orders', template='ggplot2', height=700)\n",
    "fig.update_xaxes(rangeslider_visible=True, \n",
    "    rangeselector=dict(\n",
    "        buttons=list([\n",
    "            dict(count=1, label='1m', step='month', stepmode='backward'), \n",
    "            dict(count=6, label='6m', step='month', stepmode='backward')\n",
    "            ])\n",
    "        )\n",
    "    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the shifted difference in the number of orders, we see the values increase as time increases. These differences become more pronounced from August onward.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making features, max lag 24 and rolling mean 24\n",
    "def make_features(data, max_lag, rolling_mean_size):\n",
    "    data['year'] = data.index.year\n",
    "    data['month'] = data.index.month\n",
    "    data['day'] = data.index.day\n",
    "    data['dayofweek'] = data.index.dayofweek\n",
    "    data['hour'] = data.index.hour\n",
    "\n",
    "    for lag in range(1, max_lag + 1):\n",
    "        data['lag_{}'.format(lag)] = data['num_orders'].shift(lag)\n",
    "\n",
    "    data['rolling_mean'] = (\n",
    "        data['num_orders'].shift().rolling(rolling_mean_size).mean()\n",
    "    )\n",
    "\n",
    "\n",
    "make_features(df, 24, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset to train, valid, and test\n",
    "train, test = train_test_split(df, shuffle=False, test_size=0.1, random_state=19)\n",
    "train, valid = train_test_split(train, shuffle=False, test_size=0.11, random_state=19)\n",
    "\n",
    "print('Train Dataset = ', ' Start : ', train.index.min(), '   End : ', train.index.max(), '   Difference : ', abs(train.index.min() - train.index.max()))\n",
    "print('Valid Dataset = ', ' Start : ', valid.index.min(), '   End : ', valid.index.max(), '   Difference : ', abs(valid.index.min() - valid.index.max()))\n",
    "print('Test Dataset = ', ' Start : ', test.index.min(),  '   End : ', test.index.max(), '   Difference : ', abs(test.index.min() - test.index.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping missing values from datasets \n",
    "train = train.dropna()\n",
    "valid = valid.dropna() \n",
    "test = test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting target and features\n",
    "X_train = train.drop(columns='num_orders')\n",
    "y_train = train.num_orders\n",
    "\n",
    "X_valid = valid.drop(columns='num_orders')\n",
    "y_valid = valid.num_orders \n",
    "\n",
    "X_test = test.drop(columns='num_orders')\n",
    "y_test = test.num_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(columns='num_orders')\n",
    "target = df.num_orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual of train test split\n",
    "fig, ax = plt.subplots(figsize=(25,15))\n",
    "train.num_orders.plot(ax=ax, label='Training Set', title='Data Train/Test Split')\n",
    "valid.num_orders.plot(ax=ax, label='Valid Set', color='green')\n",
    "test.num_orders.plot(ax=ax, label='Test Set')\n",
    "ax.axvline('2018-07-26 08:00:00', color='black', ls='--')\n",
    "ax.axvline('2018-08-13 14:00:00', color='black', ls='--')\n",
    "ax.legend(['Training Set', 'Test Set'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function to make features, with a max lag of 24 and a rolling mean of 24. We then split the data into three parts: train, valid, and test. We will train the models with the training data, then tune the models with the validation set. The test set is reserved for evaluating the performance of the final model we choose. Since it is crucial to have adequate training data, we limited the validation and test sets to 10% of the data each. This leaves roughly 80% of the data for training. Furthermore, with time series, we can not randomly select points in our data to split, so shuffle was set to false. This gives us the correct sequence in the order of the different sets, made evident by the last figure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**************"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear regression\n",
    "lr = LinearRegression() # initialize model constructor\n",
    "lr.fit(X_train, y_train) # train model on training set\n",
    "\n",
    "predictions_valid_lr = lr.predict(X_valid) # get model predictions on validation set\n",
    "\n",
    "result = mse(y_valid, predictions_valid_lr) ** 0.5 # calculate RMSE on validation set\n",
    "print(\"RMSE of the linear regression model on the validation set:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances of the best model\n",
    "lr_importances = lr.coef_\n",
    "\n",
    "# Create a dataframe with the feature importances and the corresponding feature names\n",
    "lr_importances_df = pd.DataFrame({'feature':X_train.columns, 'coefficients':lr.coef_})\n",
    "\n",
    "# Sort the dataframe by importance\n",
    "lr_importances_df.sort_values(by='coefficients', ascending=False, inplace=True)\n",
    "lr_importances_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rolling mean is the coefficient with the highest value among the linear regression features. We achieve an RMSE score of 34.28 with the validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "best_model = None\n",
    "best_result = 50\n",
    "best_depth = 0\n",
    "for depth in range(1, 6): # choose hyperparameter range\n",
    "    dtr = DecisionTreeRegressor(random_state=19, max_depth=depth)\n",
    "    dtr.fit(X_train, y_train) # train model on training set\n",
    "    predictions_valid_dtr = dtr.predict(X_valid) # get model predictions on validation set\n",
    "    result = mse(y_valid, predictions_valid_dtr) ** 0.5\n",
    "    if result < best_result:\n",
    "        best_model = dtr\n",
    "        best_result = result\n",
    "        best_depth = depth\n",
    "\n",
    "print(f\"RMSE of the best model on the validation set (max_depth = {best_depth}): {best_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances of the best model\n",
    "dtr_importances = dtr.feature_importances_\n",
    "\n",
    "# Create a dataframe with the feature importances and the corresponding feature names\n",
    "dtr_importances_df = pd.DataFrame({'feature':X_train.columns, 'importance':dtr.feature_importances_})\n",
    "\n",
    "# Sort the dataframe by importance\n",
    "dtr_importances_df.sort_values(by='importance', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 feature importances\n",
    "px.pie(dtr_importances_df.head(10), names='feature', values='importance', title='Top 10 Feature Importance for Decision Tree Regression', template='ggplot2', hole=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lag 24 feature is the most important in the decesion tree regression. We achieve an RMSE score of 38.54 with the validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest \n",
    "best_model = None\n",
    "best_result = 50\n",
    "best_est = 0\n",
    "best_depth = 0\n",
    "for est in range(600, 601):\n",
    "    for depth in range (100, 101):\n",
    "        rf = RandomForestRegressor(random_state=19, n_estimators=est, max_depth=depth)\n",
    "        rf.fit(X_train, y_train) # train model on training set\n",
    "        predictions_valid = rf.predict(X_valid) # get model predictions on validation set\n",
    "        result = mse(y_valid, predictions_valid) ** 0.5 # calculate RMSE on validation set\n",
    "        if result < best_result:\n",
    "            best_model = rf\n",
    "            best_result = result\n",
    "            best_est = est\n",
    "            best_depth = depth\n",
    "            \n",
    "print(\"RMSE of the best model on the validation set:\", best_result, \"n_estimators:\", best_est, \"best_depth:\", depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances of the best model\n",
    "rf_importances = best_model.feature_importances_\n",
    "\n",
    "# Create a dataframe with the feature importances and the corresponding feature names\n",
    "rf_importances_df = pd.DataFrame({'feature':X_train.columns, 'importance':rf.feature_importances_})\n",
    "\n",
    "# Sort the dataframe by importance\n",
    "rf_importances_df.sort_values(by='importance', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 feature importances\n",
    "px.pie(rf_importances_df.head(10), names='feature', values='importance', title='Top 10 Feature Importance for Random Forest Regression', template='ggplot2', hole=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lag 24 feature has the greatest importance in the random forest model. The RMSE score is 32.01. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ada Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADA Boost\n",
    "regr = AdaBoostRegressor(random_state=19, n_estimators=100)\n",
    "regr.fit(X_train, y_train)  \n",
    "\n",
    "predictions_valid_regr = regr.predict(X_valid) # get model predictions on validation set\n",
    "\n",
    "result = mse(y_valid, predictions_valid_regr) ** 0.5 # calculate RMSE on validation set\n",
    "print(\"RMSE of the ada boost regression model on the validation set:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table of feature importance\n",
    "regr_imp = [t for t in zip(features, regr.feature_importances_)]\n",
    "regr_imp_df = pd.DataFrame(regr_imp, columns=['feature', 'varimp'])\n",
    "regr_imp_df = regr_imp_df.sort_values('varimp', ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 feature importances\n",
    "px.pie(regr_imp_df.head(10), names='feature', values='varimp', title='Top 10 Feature Importance for Ada Boost Regresion', hole=.2, template='ggplot2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lag 24 feature is the most important, followed by the lag 1, among the Ada Boost model. The RMSE score is 34.89."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boost\n",
    "gbr = GradientBoostingRegressor(random_state=19, learning_rate=0.2, n_estimators=1000, verbose=100, max_depth=3)\n",
    "gbr.fit(X_train, y_train)\n",
    "\n",
    "predictions_valid_gbr = gbr.predict(X_valid)\n",
    "\n",
    "result = mse(y_valid, predictions_valid_gbr) ** 0.5 # calculate RMSE on validation set\n",
    "print(\"RMSE of the gradient boosting model on the validation set:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table of feature importance\n",
    "gbr_imp = [t for t in zip(features, gbr.feature_importances_)]\n",
    "gbr_imp_df = pd.DataFrame(gbr_imp, columns=['feature', 'varimp'])\n",
    "gbr_imp_df = gbr_imp_df.sort_values('varimp', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 feature importances\n",
    "px.pie(gbr_imp_df.head(10), names='feature', values='varimp', title='Top 10 Feature Importance for Gradient Boosting', hole=.2, template='ggplot2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lag 24 feature is the most important, followed by the lag 1, among the Gradient Boost model. The RMSE score is 35.12."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGB \n",
    "xgbr = XGBRegressor(learning_rate=0.09, n_estimators=800, eval_metric='rmse', random_state=19, max_depth=6, early_stopping_rounds=500)\n",
    "\n",
    "xgbr.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=20)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_xgbr = xgbr.predict(X_valid)\n",
    "\n",
    "result = mse(y_valid, predictions_xgbr) ** 0.5 # calculate RMSE on validation set\n",
    "print()\n",
    "print(\"RMSE of the xgbm model on the validation set:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table of feature importance\n",
    "xgbr_imp = [t for t in zip(features, xgbr.feature_importances_)]\n",
    "xgbr_imp_df = pd.DataFrame(xgbr_imp, columns=['feature', 'varimp'])\n",
    "xgbr_imp_df = xgbr_imp_df.sort_values('varimp', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 feature importances\n",
    "px.pie(xgbr_imp_df.head(10), names='feature', values='varimp', title='Top 10 Feature Importance for XG Boost', hole=.2, template='ggplot2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lag 24 feature is the most important, followed by the lag 1, among the XG Boost model. The RMSE score is 31.69."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light GBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM\n",
    "\n",
    "# Create a LightGBM dataset\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_valid = lgb.Dataset(X_valid, y_valid, reference=lgb_train)\n",
    "\n",
    "# Define the parameters for the LightGBM model\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'root_mean_squared_error',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'random_state': 19\n",
    "}\n",
    "\n",
    "# Train the LightGBM model\n",
    "lgbm = lgb.train(params, lgb_train, valid_sets=lgb_valid, num_boost_round=500, early_stopping_rounds=50)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_valid_lgbm = lgbm.predict(X_valid)\n",
    "\n",
    "result = mse(y_valid, predictions_valid_lgbm) ** 0.5 # calculate RMSE on validation set\n",
    "print()\n",
    "print(\"RMSE of the lgbm model on the validation set:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances of the trained model\n",
    "lgbm_importances = lgbm.feature_importance()\n",
    "\n",
    "# Create a dataframe with the feature importances and the corresponding feature names\n",
    "lgbm_importances_df = pd.DataFrame({'feature':X_train.columns, 'importance':lgbm_importances})\n",
    "\n",
    "# Sort the dataframe by importance\n",
    "lgbm_importances_df.sort_values(by='importance', ascending=False, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 feature importances\n",
    "px.pie(lgbm_importances_df.head(10), names='feature', values='importance', title='Top 10 Feature Importance for Light GBM Regression', hole=.2, template='ggplot2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lag 24 feature is the most important, followed by the lag 1, 3, and 17,  among the light GB model. The RMSE score is 31.51."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost\n",
    "\n",
    "catb = CatBoostRegressor(task_type='GPU', loss_function='RMSE', eval_metric='RMSE', iterations=2000, random_seed=19, early_stopping_rounds=500)\n",
    "\n",
    "catb.fit(X_train, y_train, eval_set=(X_valid, y_valid), verbose=100, use_best_model=True, plot=True)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_valid_catb = catb.predict(X_valid)\n",
    "\n",
    "result = mse(y_valid, predictions_valid_catb) ** 0.5 # calculate RMSE on validation set\n",
    "print()\n",
    "print(\"Catboost model on the test set: \")\n",
    "catb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances of the trained model\n",
    "catb_importances = catb.feature_importances_\n",
    "\n",
    "# Create a dataframe with the feature importances and the corresponding feature names\n",
    "catb_importances_df = pd.DataFrame({'feature':X_train.columns, 'importance':catb_importances})\n",
    "\n",
    "# Sort the dataframe by importance\n",
    "catb_importances_df.sort_values(by='importance', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 10 feature importances\n",
    "px.pie(catb_importances_df.head(10), names='feature', values='importance', title='Top 10 Feature Importance for Catboost Regression', hole=.2, template='ggplot2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lag 24 feature is the most important, followed by the lag 1, among the catboost model. The RMSE score is 30.42."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training classifiers\n",
    "# reg1 = RandomForestRegressor(random_state=12345, n_estimators=90, max_depth=100)\n",
    "# reg2 = GradientBoostingRegressor(random_state=19, learning_rate=0.2, n_estimators=1000, verbose=1, max_depth=3)\n",
    "reg3 = XGBRegressor(learning_rate=0.09, n_estimators=800, eval_metric='rmse', random_state=19, max_depth=6) #, early_stopping_rounds=500)\n",
    "reg4 = lgb.LGBMRegressor(objective='regression', metric='root_mean_squared_error', boosting_type='gbdt', random_state=19) #, early_stopping_rounds=50)\n",
    "reg5 = CatBoostRegressor(task_type='GPU', loss_function='RMSE', eval_metric='RMSE', iterations=2000, random_seed=19) #, early_stopping_rounds=500)\n",
    "\n",
    "ereg = VotingRegressor(estimators=[#('rf', reg1), \n",
    "                                #('gbr', reg2), \n",
    "                                ('xgb', reg3), \n",
    "                                ('lgb', reg4), \n",
    "                                ('cat', reg5)], \n",
    "                                verbose=1)\n",
    "ereg = ereg.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions_valid_ereg = ereg.predict(X_valid)\n",
    "\n",
    "result = mse(y_valid, predictions_valid_ereg) ** 0.5 # calculate RMSE on validation set\n",
    "print()\n",
    "print(\"voting regressor model on the valid set: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best model we have is the voting regressor. This model combines the three best performing models and parameters: XG boost, Light GB, and Catboost. The RMSE score is 30.78 with the validation set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making model scores dataframe\n",
    "model_scores = pd.DataFrame({'Linear Regression': 34.28, 'Decision Tree': 38.54, 'Random Forest': 32.01, 'Ada Boost': 34.89, 'Gradient Boost': 35.11, 'XG Boost': 31.69, 'Light GBM': 31.51,\n",
    "             'Catboost': 30.42, 'Voting Regressor': 30.78}, index={'RMSE'})\n",
    "model_scores = model_scores.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model RMSE scores\n",
    "px.scatter(model_scores, title='Model RMSE Scores', template='ggplot2', color=model_scores.index, size='RMSE', y='RMSE', size_max=30, labels={'index': 'Model'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This figure compares the RMSE scores of the various models. The three best performing models are the Gradient boost, XG boost, and Catboost. Finally, the Voting regressor achieved a great score.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine validation set with training set\n",
    "X_full = pd.concat([X_train, X_valid])\n",
    "y_full = pd.concat([y_train, y_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training classifiers\n",
    "# reg1 = RandomForestRegressor(random_state=12345, n_estimators=90, max_depth=100)\n",
    "# reg2 = GradientBoostingRegressor(random_state=19, learning_rate=0.2, n_estimators=1000, verbose=1, max_depth=3)\n",
    "reg3 = XGBRegressor(learning_rate=0.09, n_estimators=800, eval_metric='rmse', random_state=19, max_depth=6) #, early_stopping_rounds=500)\n",
    "reg4 = lgb.LGBMRegressor(objective='regression', metric='root_mean_squared_error', boosting_type='gbdt', random_state=19) #, early_stopping_rounds=50)\n",
    "reg5 = CatBoostRegressor(task_type='GPU', loss_function='RMSE', eval_metric='RMSE', iterations=2000, random_seed=19) #, early_stopping_rounds=500)\n",
    "\n",
    "final = VotingRegressor(estimators=[#('rf', reg1), \n",
    "                                #('gbr', reg2), \n",
    "                                ('xgb', reg3), \n",
    "                                ('lgb', reg4), \n",
    "                                ('cat', reg5)], \n",
    "                                verbose=1)\n",
    "final = final.fit(X_full, y_full)\n",
    "\n",
    "# Make predictions on the test set\n",
    "final_predictions = final.predict(X_test)\n",
    "\n",
    "result = mse(y_test, final_predictions) ** 0.5 # calculate RMSE on validation set\n",
    "print()\n",
    "print(\"voting regressor model on the test set: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final model we chose was the voting regressor. We achieved a RMSE score of 41.24 with the test set. This model performs better than the required RMSE score of 48. Therefore, we have accurately predicted the future number of orders. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we succeeded in providing a model for Sweet Lift Taxi to predict the number of orders of the next hour. The target metric for our model was an RMSE score under 48. Our final model was a voting regressor, with a final RMSE of 46.47 with the test data set. Therefore, Sweet Lift can accommodate drivers with a model that accurately predicts future number of orders.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  4 2022, 05:22:27) [MSC v.1916 64 bit (AMD64)]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "4f2b0cfe12c109b58467a02dc33230d9e6228c23b43020d2941c37e2a7dfdd3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
